{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# ! pip install langchain langchain-community -q\n",
    "# ! pip install sentence-transformers langchain_huggingface -q\n",
    "# ! pip install datasets -q\n",
    "#! pip install chromadb -q\n",
    "# ! pip install langchain_ollama -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ARITRA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "#from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "knowledge_base = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n",
    "knowledge_base = knowledge_base.filter(lambda row: row[\"source\"].startswith(\"huggingface/transformers\"))\n",
    "\n",
    "source_docs = [\n",
    "    Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"].split(\"/\")[1]})\n",
    "    for doc in knowledge_base\n",
    "]\n",
    "\n",
    "### Creating Chunks using RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    add_start_index=True,\n",
    "    strip_whitespace=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "new_docs = text_splitter.split_documents(documents=source_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  BGE Embddings\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)\n",
    "\n",
    "### Populate Vector DB\n",
    "\n",
    "db = Chroma.from_documents(new_docs,embeddings,persist_directory='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'transformers', 'start_index': 31045}, page_content='|                  [Transformer-XL](model_doc/transfo-xl)                  |       ✅        |         ✅         |      ❌      |\\n|                         [TrOCR](model_doc/trocr)                         |       ✅        |         ❌         |      ❌      |\\n|                          [TVLT](model_doc/tvlt)                          |       ✅        |         ❌         |      ❌      |'),\n",
       " Document(metadata={'source': 'transformers', 'start_index': 3309}, page_content='### Axial Positional Encodings'),\n",
       " Document(metadata={'source': 'transformers', 'start_index': 29893}, page_content='|           [SwitchTransformers](model_doc/switch_transformers)            |       ✅        |         ❌         |      ❌      |\\n|                            [T5](model_doc/t5)                            |       ✅        |         ✅         |      ✅      |\\n|                        [T5v1.1](model_doc/t5v1.1)                        |       ✅        |         ✅         |      ✅      |'),\n",
       " Document(metadata={'source': 'transformers', 'start_index': 3245}, page_content=\"with \\\\\\\\(Q\\\\\\\\), \\\\\\\\(K\\\\\\\\) and \\\\\\\\(V\\\\\\\\) are matrices of shape `seq_len x hidden_size` named query, key and value (they are actually bigger matrices with a batch dimension and an attention head dimension but we're only interested in the last two, which is where the matrix product is taken, so for the sake of simplicity we only consider those two)\")]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke('Tell me about Key matrices in Transformer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSeek-R1 model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm=ChatOllama(\n",
    "    model='deepseek-r1:1.5b',\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions to Reasoning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke('Why we need of critic model in Reinforcement learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to understand why we need a critic model in reinforcement learning. I remember that RL is all about an agent learning from its environment through trial and error, right? The agent interacts with the world, receives rewards or penalties, and tries to maximize its cumulative reward.\n",
      "\n",
      "I think there's something called a policy gradient method where the agent directly learns the policy by adjusting its actions based on gradients of some loss function. But I also recall that this might not be enough because it doesn't account for the state of the environment fully. So maybe the critic model comes into play here.\n",
      "\n",
      "The user mentioned that without a critic, the agent might make suboptimal decisions or get stuck in local optima. That makes sense because if the agent only learns from immediate rewards, it could take risky actions that lead to worse outcomes later on. For example, imagine an agent playing a game where taking a wrong move now leads to losing points but gives more points next time. The policy gradient might learn the immediate reward, so it would choose that wrong move even if it's bad in the long run.\n",
      "\n",
      "So, what exactly is a critic model? From what I remember, a critic evaluates the actions taken by the agent and provides feedback based on the expected return from the state. It's like having a separate function or network that assesses how good a particular action was given the current state of the environment. This helps in making better decisions because it considers the overall future reward, not just what happens next.\n",
      "\n",
      "I think there are two main types of models used as critics: value functions and Q-functions. The value function estimates the expected return starting from a specific state under a certain policy. In contrast, the Q-function directly estimates the action-value for a given state-action pair. Using a Q-function is more direct because it captures the immediate reward plus what's expected in the future.\n",
      "\n",
      "So, if we only use a policy gradient method without a critic, the agent might make locally optimal decisions that don't consider the long-term consequences. That's why having a critic ensures that the agent's actions are not just good now but also lead to better overall rewards over time. This is especially important in complex environments where immediate gains can be misleading.\n",
      "\n",
      "The user also mentioned that without a critic, the learning process might get stuck because the agent isn't getting enough information about how its actions affect future states and rewards. The critic provides this feedback through the loss function, guiding the policy gradient to adjust its strategy accordingly.\n",
      "\n",
      "I'm trying to think of real-world examples where a critic model is crucial. Maybe in video games like AlphaGo or more modern games with complex mechanics. Even if I don't know all the specifics, understanding that without a critic the agent can't learn the long-term consequences seems important.\n",
      "\n",
      "In summary, the critic model helps evaluate actions over time, ensuring that the agent doesn't make decisions based solely on immediate rewards but considers the broader picture. This leads to more robust and effective learning in reinforcement learning tasks.\n",
      "</think>\n",
      "\n",
      "The critic model is essential in reinforcement learning (RL) because it evaluates actions not just immediately but also considering their long-term consequences. Here's a structured explanation of its importance:\n",
      "\n",
      "1. **Immediate vs. Long-Term Gains**: Without a critic, the agent might make locally optimal decisions based solely on immediate rewards. This can lead to suboptimal choices that result in worse overall outcomes later.\n",
      "\n",
      "2. **Feedback Through Loss Function**: The critic provides feedback by evaluating actions against expected future returns. This loss function ensures that the policy gradient adjusts its strategy based on how well it aligns with long-term rewards.\n",
      "\n",
      "3. **Value vs. Q-Functions**: While both value functions and Q-functions aim to estimate future rewards, the Q-function directly estimates action values from state-action pairs. The critic model is crucial for this direct evaluation.\n",
      "\n",
      "4. **Avoiding Local Optima**: The critic helps avoid getting stuck in local optima by ensuring that decisions are balanced against their long-term implications, guiding the agent toward globally optimal strategies.\n",
      "\n",
      "5. **Real-World Applications**: Examples include complex games and environments where immediate gains can be misleading. Without a critic, learning would be insufficiently informed about future states and rewards.\n",
      "\n",
      "In conclusion, the critic model is vital for RL agents to learn effective policies by considering long-term consequences, ensuring robust and efficient learning across various tasks.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Chain Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "retrieval_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Reasoning RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = retrieval_chain.invoke('Tell me about Key matrices in Transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, I need to figure out how to answer the question \"Tell me about Key matrices in Transformer.\" Let's start by understanding what the user is asking. They want a clear explanation of key matrices within the context provided.\n",
      "\n",
      "Looking at the document content, there are several documents listed with their source and some page content. The first document mentions [Transformer-XL] and provides details on query, key, and value matrices. It says that these are named as Q, K, V respectively, each being a matrix of shape seq_len x hidden_size. They mention that they're bigger matrices but only use the last two for attention calculations.\n",
      "\n",
      "The second document is about Axial Positional Encodings, which might not be directly relevant since it's more about encoding positions rather than key matrices in Transformers.\n",
      "\n",
      "The third document lists SwitchTransformers and T5 models with their respective page content. It mentions that Q, K, V are query, key, value matrices of shape seq_len x hidden_size each. They also note that these have a batch dimension and attention head dimensions but focus on the last two for attention.\n",
      "\n",
      "So, putting this together, the key matrix is one of three (Q, K, V) used in the Transformer architecture. Each has a shape of [batch_size x sequence_length x hidden_size]. The key matrix specifically refers to K, which is used in the attention mechanism to compute keys from the input sequence. The value matrix is V.\n",
      "\n",
      "I should make sure to explain that each of these matrices is crucial for the model's operation, especially how they contribute to the attention process and the overall computation during training or inference.\n",
      "</think>\n",
      "\n",
      "In the Transformer architecture, key matrices (K) are a critical component used in the attention mechanism. Each key matrix is a 3D tensor with dimensions [batch_size x sequence_length x hidden_size], where:\n",
      "- **batch_size**: The number of examples in the batch.\n",
      "- **sequence_length**: The length of the input sequence.\n",
      "- **hidden_size**: The dimension of each token representation.\n",
      "\n",
      "The key matrices are used to compute keys from the input sequence, which is a crucial step in attention mechanisms. Each element in the key matrix represents a feature vector associated with a position in the input sequence. These vectors help the model focus on different parts of the input when performing multi-head attention.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = retrieval_chain.invoke('Tell me details of transformer training process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'transformers', 'start_index': 2357}, page_content=\"<Tip>\\n\\nBefore you go further, it is good to have some basic knowledge of the original Transformer architecture. Knowing how encoders, decoders, and attention work will aid you in understanding how different Transformer models work. If you're just getting started or need a refresher, check out our [course](https://huggingface.co/course/chapter1/4?fw=pt) for more information! \\n\\n</Tip>\\n\\n## Speech and audio\"),\n",
       " Document(metadata={'source': 'transformers', 'start_index': 31045}, page_content='|                  [Transformer-XL](model_doc/transfo-xl)                  |       ✅        |         ✅         |      ❌      |\\n|                         [TrOCR](model_doc/trocr)                         |       ✅        |         ❌         |      ❌      |\\n|                          [TVLT](model_doc/tvlt)                          |       ✅        |         ❌         |      ❌      |'),\n",
       " Document(metadata={'source': 'transformers', 'start_index': 3486}, page_content='### Supervised training\\n\\n```python\\n>>> from transformers import PLBartForConditionalGeneration, PLBartTokenizer'),\n",
       " Document(metadata={'source': 'transformers', 'start_index': 2264}, page_content=\"Once you've opened a new model request, the first step is to get familiar with 🤗 Transformers if you aren't already!\\n\\n## General overview of 🤗 Transformers\")]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke('Tell me details of transformer training process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out how the Transformer training process works based on the given context. Let's start by reading through the provided document.\n",
      "\n",
      "First, there are three documents mentioned. The first one talks about understanding the Transformer architecture for basic knowledge. It mentions encoders, decoders, and attention mechanisms, which is helpful but not directly related to the training process details. The second document discusses a specific model called Transformer-XL, mentioning that some parts work correctly (like [Transformer-XL](model_doc/transfo-xl)) while others don't ([TrOCR], [TVLT]). This part seems more about model functionality than training.\n",
      "\n",
      "The third document is about Supervised training with an example using PLBartForConditionalGeneration and PLBartTokenizer. It says once you open a new model request, the first step is to familiarize yourself with Hugging Face Transformers if you're not already. But this doesn't address the training process itself beyond that.\n",
      "\n",
      "Wait, maybe I'm missing something. The actual question is about the training process details. So perhaps the context isn't directly answering it. Maybe the user wants an overview of how training works in general using Transformers, but the provided documents don't cover that.\n",
      "\n",
      "I should check if there's any mention of training steps or specific techniques. From what I see, the second document doesn't provide much on training. The third is about a model setup, not the actual training process itself.\n",
      "\n",
      "So perhaps the answer is that the context doesn't include detailed information about the training process. It focuses more on architecture and model specifics rather than how the training happens.\n",
      "</think>\n",
      "\n",
      "The provided context does not include detailed information about the Transformer training process. It primarily discusses understanding the architecture of Transformers, mentioning specific models like Transformer-XL and their limitations, as well as an introduction to model setup using a specific framework (PLBartForConditionalGeneration). However, it does not provide insights into how the actual training occurs or any methodologies involved in the training process.\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "The context provided does not include detailed information about the Transformer training process. It focuses on explaining the architecture of Transformers and introducing specific models but does not delve into the methods used for training these models.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out how to explain the BERT model based on the given context. Let's start by looking at the document entries provided.\n",
      "\n",
      "First, there are multiple documents listed with metadata that includes a source and a starting index. Each document seems to be part of a larger collection from the transformers library, probably referring to various models like BERT, BPT touched on them in different ways or versions.\n",
      "\n",
      "Looking through each entry:\n",
      "\n",
      "1. The first document talks about Fine-tuning BERT on SQuAD1.0. That's a specific task where BERT is trained to answer questions based on labeled text data.\n",
      "2. Then there are other model names like ALBERT, BART, Bert Generation, BioGpt, Blenderbot, and others. These seem to be different variants or perhaps related models that build upon or use the core concepts of BERT.\n",
      "\n",
      "The key point I notice is that all these documents mention \"[BERT](../model_doc/bert)\" in their page content. This repetition suggests that BERT is a central model used across multiple variations and extensions within the transformers library. It's likely that each document is discussing different aspects or versions of the BERT model, perhaps focusing on specific tasks or improvements.\n",
      "\n",
      "I should note that while all documents refer to BERT, they might be exploring different configurations, applications, or enhancements beyond the base BERT model. For example, some could focus on Japanese variants like BertJapanese, others might explore more complex models for tasks like question answering (QA), which is what SQuAD1.0 involves.\n",
      "\n",
      "Additionally, I see that there's a mention of [Bert Generation](model_doc/bert-generation) and other related topics, indicating that the BERT model is being used in areas beyond just classification or text understanding, such as generating text or addressing specific tasks like question answering.\n",
      "\n",
      "So putting it all together, the main point is that BERT is a versatile model within the transformers library, capable of various tasks including text classification, question answering (SQuAD1.0), and even text generation. Each document in this context likely discusses different configurations or applications of BERT to specific problems or datasets.\n",
      "</think>\n",
      "\n",
      "The BERT model is a versatile and widely used deep learning architecture that has made significant contributions across various natural language processing tasks. Here's an organized summary based on the provided context:\n",
      "\n",
      "1. **Core Model**: The base version of the BERT model, often referred to as [BERT](../model_doc/bert), is a transformer-based architecture designed for efficient pre-training and fine-tuning.\n",
      "\n",
      "2. **Applications Across Tasks**:\n",
      "   - **Text Classification (SQuAD1.0)**: BERT has been fine-tuned on datasets like SQuAD1.0, where it processes labeled text to answer questions.\n",
      "   - **Question Answering**: Enhanced versions of BERT have focused on specific tasks such as question answering, which is what SQuAD1.0 targets.\n",
      "   - **Other Models and Variants**: The context also includes other models like ALBERT, BART, Bert Generation, BioGpt, Blenderbot, and others, which are variants or extensions of the core BERT model.\n",
      "\n",
      "3. **Task-Specific Focus**:\n",
      "   - Different configurations of BERT have been explored for specific tasks, such as generating text (BertGeneration) and handling more complex models like BioGPT.\n",
      "   - The library provides tools to adapt BERT for various applications beyond basic text understanding.\n",
      "\n",
      "4. **Versatility**: Each document in the context likely discusses different aspects or versions of the BERT model, focusing on specific configurations or extensions tailored for particular tasks or datasets.\n",
      "\n",
      "In summary, while all documents reference the BERT model, they explore its versatility across various tasks and applications within the transformers library.\n"
     ]
    }
   ],
   "source": [
    "result = retrieval_chain.invoke('Tell me about the BERT Model')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
